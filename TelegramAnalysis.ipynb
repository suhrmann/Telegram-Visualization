{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse Telegram Chat\n",
    "\n",
    "Anlayse and visualize the exported messages from `Telegram Desktop`\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Telegram Chat Export Tool\n",
    "\n",
    "For documentation how to export chats head over to the doc **with video** at [blog of Telegram](https://www.telegram.org/blog/export-and-more)\n",
    "\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The directory containing the data export\n",
    "# EDIT the date to match your export!!!\n",
    "\n",
    "from pathlib import Path\n",
    "HOME_PATH = str(Path.home()) # Python 3.5+\n",
    "EXPORT_FOLDER = HOME_PATH + \"/Downloads/Telegram Desktop/ChatExport_20_01_2020/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the generated charts to this folder\n",
    "OUTPUT_DIR = \"CHARTS/\" # Note the \"/\"\" at the end!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Telegram Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Jan-Eike Golenia  @jagoleni\n",
    "Source: https://github.com/jagoleni/tele-data/blob/master/tele-data.py\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "from lxml import etree\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def parse_file(html_string):\n",
    "    data = []\n",
    "    parser = etree.HTMLParser()\n",
    "    root = etree.HTML(html_string)\n",
    "    for element in root.iter():\n",
    "        if \"id\" in element.attrib:\n",
    "            message = {}\n",
    "            message[\"message_id\"] = element.attrib[\"id\"]\n",
    "            for child in element.getchildren():\n",
    "                if element.attrib[\"class\"] == \"message service\" and \\\n",
    "                    child.attrib[\"class\"] == \"body details\":\n",
    "                        message[\"text\"] = child.text.strip()\n",
    "                        message['type'] = 'service_message'\n",
    "                if child.attrib[\"class\"] == \"body\":\n",
    "                    for grandchild in child.getchildren():\n",
    "                        if grandchild.attrib[\"class\"] == \"from_name\":\n",
    "                            name = grandchild.text.strip()\n",
    "                            message[\"name\"] = name\n",
    "                        if grandchild.attrib[\"class\"] == \"pull_right date details\":\n",
    "                            message['timestamp'] = grandchild.attrib[\"title\"]\n",
    "                        if grandchild.attrib[\"class\"] == \"text\":\n",
    "                            message['text'] = grandchild.text.strip()\n",
    "                            message['type'] = 'text'\n",
    "                        if grandchild.attrib[\"class\"] == \"forwarded body\":\n",
    "                            message['type'] = \"forwarded_message\"\n",
    "                        if grandchild.attrib[\"class\"] == \"media_wrap clearfix\":\n",
    "                            message['type'] = \\\n",
    "                                grandchild.getchildren()[0].attrib[\"class\"].split()[-1]\n",
    "            if element.attrib[\"class\"] == \"message default clearfix joined\":\n",
    "                message[\"joined_message\"] = True\n",
    "                message[\"name\"] = name\n",
    "            if element.attrib[\"class\"] == \"message default clearfix\":\n",
    "                message[\"joined_message\"] = False\n",
    "            data.append(message)\n",
    "    return data\n",
    "\n",
    "data = []\n",
    "filecount = 0\n",
    "for fname in os.listdir(EXPORT_FOLDER):\n",
    "    fpath = os.path.join(EXPORT_FOLDER, fname)\n",
    "    if os.path.isfile(fpath) and os.path.splitext(fpath)[-1] == \".html\":\n",
    "        with open(fpath, encoding='utf8') as f:\n",
    "            # print(\"Reading\", fname, \"...\")\n",
    "            data += parse_file(f.read())\n",
    "            filecount += 1\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], format=\"%d.%m.%Y %H:%M:%S\")\n",
    "#plot_message_count_by_name(df)\n",
    "\n",
    "print(\"Queried \", len(df.index), \"raw messages from \", filecount, \"files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out all service_message\n",
    "messages = df.loc[df['type'] == \"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display parsed messages\n",
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize\n",
    "\n",
    "To setup Plotly with Jupyter Lab follow the instructions at [plotly > Getting Started](https://plot.ly/python/getting-started/) - especially the [Jupyter Lab instructions](https://plot.ly/python/getting-started/#jupyterlab-support-python-35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install wordcloud\n",
    "#!conda install -c conda-forge wordcloud -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Plotly-Test\n",
    "import plotly.graph_objects as go\n",
    "fig = go.Figure(data=go.Bar(y=[2, 3, 1]))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total text messages: \", len(messages.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Messages by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date\n",
    "\n",
    "dates = pd.DataFrame( messages['timestamp'].dt.date )\n",
    "date_counts = pd.DataFrame( dates.stack().value_counts(sort=False) )\n",
    "\n",
    "data = [go.Bar(x=date_counts.index, y=date_counts[0])]\n",
    "\n",
    "# Plot chart\n",
    "fig = go.Figure(data)\n",
    "fig.show()\n",
    "# Save to file\n",
    "filename = \"Telegram_by_date.png\"\n",
    "fig.write_image(OUTPUT_DIR + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year\n",
    "\n",
    "years = pd.DataFrame( messages['timestamp'].dt.year )\n",
    "year_counts = pd.DataFrame( years.stack().value_counts(sort=False) )\n",
    "\n",
    "data = [go.Bar(x=year_counts.index, y=year_counts[0])]\n",
    "\n",
    "# Plot chart\n",
    "fig = go.Figure(data)\n",
    "fig.show()\n",
    "# Save to file\n",
    "filename = \"Telegram_by_year.png\"\n",
    "fig.write_image(OUTPUT_DIR + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Messages by Time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time\n",
    "\n",
    "hours = pd.DataFrame( pd.DatetimeIndex(messages['timestamp']).hour )\n",
    "\n",
    "time_counts = pd.DataFrame( hours.stack().value_counts(sort=False) ) \n",
    "\n",
    "data = [go.Bar(x=time_counts.index, y=time_counts[0])]\n",
    "\n",
    "# Plot chart\n",
    "fig = go.Figure(data)\n",
    "fig.show()\n",
    "# Save to file\n",
    "filename = \"Telegram_by_time.png\"\n",
    "fig.write_image(OUTPUT_DIR + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lbls = list( time_counts.index.astype(str) )\n",
    "data = [go.Pie(labels=lbls, values=time_counts[0], hole=.3)] #, marker_colors=night_colors)]\n",
    "\n",
    "fig.update_traces(textposition='inside', textinfo='label')\n",
    "fig = go.Figure(data=data)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_texts = messages['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for txt in message_texts:\n",
    "    word_tokens = txt.split()\n",
    "    words += word_tokens\n",
    "    \n",
    "message_text_flat = \" \".join(words)\n",
    "\n",
    "print(\"Total words: \", len(words))\n",
    "print(\"Words per message: \", len(words) / len(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on: https://amueller.github.io/word_cloud/auto_examples/single_word.html#sphx-glr-auto-examples-single-word-py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "#x, y = np.ogrid[:300, :300]\n",
    "x, y = np.ogrid[:1000, :1000]\n",
    "\n",
    "#mask = (x - 150) ** 2 + (y - 150) ** 2 > 130 ** 2\n",
    "mask = (x - 500) ** 2 + (y - 500) ** 2 > 400 ** 2\n",
    "mask = 255 * mask.astype(int)\n",
    "\n",
    "\n",
    "wc = WordCloud(background_color=\"white\", repeat=True, mask=mask)\n",
    "wc.generate(message_text_flat)\n",
    "\n",
    "# store to file\n",
    "wc.to_file(OUTPUT_DIR + \"Telegram_cloud.png\")\n",
    "\n",
    "# show\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emoji Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extended Count emojis\n",
    "# Also counts \"Emoji words\" (multiple Emojis in a row)\n",
    "\n",
    "import emoji\n",
    "\n",
    "TEST_STR = \"Get Emoji — All Emojis to ️ Copy and 📋 Paste 👌\"\n",
    "\n",
    "emoji_total_count = 0\n",
    "emoji_word_total_count = 0\n",
    "emoji_counts = {}\n",
    "emoji_word_counts = {}\n",
    "emoji_word = \"\"\n",
    "#for idx, ch in message_text_flat:\n",
    "for idx in range(len(message_text_flat)):\n",
    "    ch = message_text_flat[idx]\n",
    "    if ch in emoji.unicode_codes.UNICODE_EMOJI:\n",
    "        # Character is an Emoji\n",
    "        emoji_total_count += 1\n",
    "        emoji_word += ch # Append current Emoji to Emoji word\n",
    "        if ch in emoji_counts:\n",
    "            # Increment existing entry\n",
    "            emoji_counts[ch] += 1\n",
    "        else:\n",
    "            # Create new enty in dictionary\n",
    "            emoji_counts[ch] = 1\n",
    "    elif ch == \" \":\n",
    "         # Ignore spaces between emojis\n",
    "        ;\n",
    "    elif len(emoji_word) > 1:\n",
    "        # Characters (etc.) terminate Emoji word\n",
    "        emoji_word_total_count += 1\n",
    "        if emoji_word in emoji_counts:\n",
    "            # Increment existing entry\n",
    "            emoji_word_counts[emoji_word] += 1\n",
    "        else:\n",
    "            # Create new enty in dictionary\n",
    "            emoji_word_counts[emoji_word] = 1\n",
    "        emoji_word = \"\" # Start over Emoji word\n",
    "\n",
    "print(\"Found\", emoji_total_count, \"emojis.\")\n",
    "print(\"Found\", emoji_word_total_count, \"emoji words.\")\n",
    "# emoji_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Emojis\n",
    "\n",
    "emoji_counts_df = pd.DataFrame( list(emoji_counts.items()) ) \n",
    "# Sort by frequency / count\n",
    "emoji_counts_df.sort_values(1, ascending=False, inplace=True) # Sort by col\n",
    "\n",
    "data = [go.Bar(x=emoji_counts_df[0], y=emoji_counts_df[1])]\n",
    "\n",
    "# Plot chart\n",
    "fig = go.Figure(data)\n",
    "fig.show()\n",
    "# Save to file\n",
    "filename = \"Telegram_emojis.png\"\n",
    "fig.write_image(OUTPUT_DIR + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emoji Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Emoji words\n",
    "\n",
    "emoji_word_counts_df = pd.DataFrame( list(emoji_word_counts.items()) ) \n",
    "# Sort by length of Emoji word\n",
    "emoji_word_counts_df['length'] = emoji_word_counts_df[0].str.len()\n",
    "emoji_word_counts_df.sort_values('length', ascending=False, inplace=True)\n",
    "\n",
    "data = [go.Bar(x=emoji_word_counts_df[0], y=emoji_word_counts_df[1])]\n",
    "\n",
    "# Plot chart\n",
    "fig = go.Figure(data)\n",
    "fig.show()\n",
    "# Save to file\n",
    "filename = \"Telegram_emoji_words.png\"\n",
    "fig.write_image(OUTPUT_DIR + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
